{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff50a3f3",
   "metadata": {},
   "source": [
    "# # Web-Search SDK – End-to-End Demo (V2)\n",
    "# \n",
    "# This notebook shows **how to install, configure and use** the SDK to pull\n",
    "# publicly-available web signals – from simple keyword extraction to\n",
    "# paywall handling and Twitter scraping – in **under 3 minutes**.\n",
    "# \n",
    "# <https://github.com/Gregory-307/web-search-sdk>\n",
    "# \n",
    "# ---\n",
    "# **Tip** Set `OFFLINE_MODE=1` to run everything against fixture HTML – great\n",
    "# for CI or airplane mode!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416bc395",
   "metadata": {},
   "source": [
    "# ## 1  Environment Setup  \n",
    "# This cell **bootstraps a completely fresh Colab**:\n",
    "# 1. Installs the Web-Search SDK in *editable* mode (if missing).\n",
    "# 2. Installs the Playwright Python package (if missing).\n",
    "# 3. Downloads headless browser binaries (idempotent).\n",
    "#\n",
    "# Feel free to run it multiple times – each step is safe and will be skipped\n",
    "# when already satisfied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b0920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, pathlib, os, sys\n",
    "\n",
    "# Clone repo when notebook is opened outside the repository tree (e.g. Colab)\n",
    "REPO_URL = \"https://github.com/Gregory-307/web-search-sdk.git\"\n",
    "REPO_DIR = pathlib.Path(\"web-search-sdk\")\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    print(\"Cloning repo …\")\n",
    "    subprocess.check_call([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, str(REPO_DIR)])\n",
    "\n",
    "ROOT = REPO_DIR.resolve()\n",
    "\n",
    "# Install SDK (editable) + Playwright package & browsers – always runs, safe and idempotent\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", f\"{ROOT}[browser]\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"playwright\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"playwright\", \"install\", \"--with-deps\"], stdout=subprocess.DEVNULL)\n",
    "\n",
    "# Make repo importable\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "print(\"✅ Environment ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39bdc0c",
   "metadata": {},
   "source": [
    "# ## 2  Quick Smoke Test\n",
    "# Verifies that the freshly-installed package is importable and that the\n",
    "# built-in `smoke_test.py` script runs without network access.  \n",
    "# **Expected output**: version string + a list of top-tokens for the term you\n",
    "# pass on the CLI (defaults inside the script).  This takes <2 s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import runpy, importlib.metadata as md, pathlib\n",
    "print(\"web_search_sdk version:\", md.version(\"web-search-sdk\"))\n",
    "runpy.run_path(str((pathlib.Path.cwd() / \"web-search-sdk\" / \"smoke_test.py\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23a174",
   "metadata": {},
   "source": [
    "# ## 3  ScraperContext Cheatsheet\n",
    "# `ScraperContext` is the _single_ configuration object shared by every helper\n",
    "# in the SDK.  It controls:\n",
    "# • HTTP headers, timeouts & retries  \n",
    "# • Proxy / custom User-Agent pools  \n",
    "# • Whether to launch a headless browser fallback (Selenium / Playwright)  \n",
    "# • Verbose logging for debugging\n",
    "#\n",
    "# The cell below instantiates three ready-made contexts to reuse in later\n",
    "# examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.base import ScraperContext\n",
    "ctx_http  = ScraperContext()\n",
    "ctx_selen = ScraperContext(use_browser=True, browser_type=\"selenium\", debug=False)\n",
    "ctx_play  = ScraperContext(use_browser=True, browser_type=\"playwright_stealth\")\n",
    "ctx_http, ctx_selen, ctx_play\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c46b9",
   "metadata": {},
   "source": [
    "# ## Part A – Scraping Helpers\n",
    "# ### A1 Keyword Extractors – DuckDuckGo SERP\n",
    "# Primary engine: zero CAPTCHA risk, lightweight HTML.  Returns top-N tokens\n",
    "# from the SERP snippets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc74875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.duckduckgo_web import duckduckgo_top_words\n",
    "print(await duckduckgo_top_words(\"bitcoin swing\", ctx_http, top_n=15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ec19e3",
   "metadata": {},
   "source": [
    "# ### A2 Keyword Extractors – Wikipedia Page\n",
    "# Low-latency and highly reliable.  Good sanity-check source for any term.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f340e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.wikipedia import wikipedia_top_words\n",
    "print(await wikipedia_top_words(\"bitcoin\", ctx_http, top_n=15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15862721",
   "metadata": {},
   "source": [
    "# ### A3 Semantic Expansion – RelatedWords\n",
    "# Expands the seed term via semantic similarity API; useful for idea\n",
    "# generation or keyword expansion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.related import related_words\n",
    "_syn = await related_words(\"bitcoin\", ctx_http)\n",
    "print(_syn[:15])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b671810",
   "metadata": {},
   "source": [
    "# ### A4 Keyword Extractors – Google News RSS\n",
    "# Headlines surface fresh jargon earlier than static pages – this parser\n",
    "# extracts frequent tokens from the Google News RSS feed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.news import google_news_top_words\n",
    "print(await google_news_top_words(\"bitcoin\", ctx_http, top_n=15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c713afa",
   "metadata": {},
   "source": [
    "# ### A5 Google SERP Fallback *(optional)*\n",
    "# Heavy and may hit CAPTCHA – **runs by default**. Set `DISABLE_GOOGLE=1` to skip in CI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d8068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Skip only when explicitly disabled\n",
    "if os.getenv(\"DISABLE_GOOGLE\") == \"1\":\n",
    "    print(\"[skipped] DISABLE_GOOGLE env var set\")\n",
    "else:\n",
    "    from web_search_sdk.scrapers.google_web import google_web_top_words\n",
    "    print(await google_web_top_words(\"bitcoin swing\", ctx_play, top_n=15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a46de",
   "metadata": {},
   "source": [
    "# ### A6 Paywall Article Retrieval – Bloomberg/CNBC\n",
    "# Shows automatic switch to Playwright when a JS-heavy paywall blocks simple\n",
    "# HTTP.  Trimmed article body is printed to keep output concise.  Skips when\n",
    "# `OFFLINE_MODE=1`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe8e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.paywall import fetch_bloomberg\n",
    "if os.getenv(\"OFFLINE_MODE\"):\n",
    "    print(\"[skipped] Offline mode – using fixtures\")\n",
    "else:\n",
    "    art = await fetch_bloomberg(\"https://www.bloomberg.com/news/articles/2023-12-01/bitcoin-price-breaks-40k\", ctx_play)\n",
    "    print(art[:400], \"…\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f94246",
   "metadata": {},
   "source": [
    "# ## Part B – Social Media\n",
    "# ### B1 Twitter Login & Timeline Scrape *(optional)*\n",
    "# **Requires** env vars `TW_EMAIL`, `TW_PASS`.  Runs automatically when creds are present; otherwise skipped.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run when credentials are provided (no extra flag needed)\n",
    "tw_user = os.getenv(\"TW_EMAIL\")\n",
    "tw_pass = os.getenv(\"TW_PASS\")\n",
    "\n",
    "if tw_user and tw_pass:\n",
    "    # Minimal inline Playwright demo (pseudo-code for brevity)\n",
    "    from playwright.async_api import async_playwright  # type: ignore\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(\"https://twitter.com/login\")\n",
    "        await page.fill(\"input[name='text']\", tw_user)\n",
    "        await page.press(\"input[name='text']\", \"Enter\")\n",
    "        await page.fill(\"input[name='password']\", tw_pass)\n",
    "        await page.press(\"input[name='password']\", \"Enter\")\n",
    "        await page.wait_for_selector(\"article\")\n",
    "        html = await page.content()\n",
    "        print(html[:500], \"…\")\n",
    "        await browser.close()\n",
    "else:\n",
    "    print(\"[skipped] Provide TW_EMAIL and TW_PASS env vars to enable Twitter demo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924146d",
   "metadata": {},
   "source": [
    "# ## Part C – Toolkit Helpers\n",
    "# ### C1 Output Utilities (JSON/CSV)\n",
    "# Lightweight helpers that write structured results to JSON/CSV.  Both create\n",
    "# parent folders automatically and support **append** mode for easy logging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.utils.output import to_json, to_csv\n",
    "import json, pathlib\n",
    "pathlib.Path(\"out\").mkdir(exist_ok=True)\n",
    "\n",
    "json_path = \"out/demo_tokens.json\"\n",
    "to_json([\"btc\", \"eth\", \"doge\"], json_path, append=False)\n",
    "print(\"Wrote\", json_path, \"bytes:\", pathlib.Path(json_path).stat().st_size)\n",
    "\n",
    "csv_path = \"out/demo_tokens.csv\"\n",
    "to_csv([{\"term\": \"btc\", \"hits\": 120}], csv_path, append=False)\n",
    "print(\"Wrote\", csv_path, \"bytes:\", pathlib.Path(csv_path).stat().st_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac34a53",
   "metadata": {},
   "source": [
    "# ### C2 Text Helpers\n",
    "# Tokenisation + stop-word removal + frequency counter in one line each.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.utils.text import tokenise, remove_stopwords, most_common\n",
    "raw = \"Bitcoin's all-time high price sparks FOMO!\"\n",
    "print(\"tokens:\", tokenise(raw))\n",
    "print(\"no stopwords:\", remove_stopwords(tokenise(raw)))\n",
    "print(\"top:\", most_common(tokenise(raw), 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9584dbab",
   "metadata": {},
   "source": [
    "# ### C3 Rate-Limit Decorator\n",
    "# Async token-bucket decorator – guarantees you never exceed X calls / period.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101ef45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from web_search_sdk.utils.rate_limit import rate_limited\n",
    "\n",
    "@rate_limited(calls=2, period=1.0)\n",
    "async def _ping(i: int):\n",
    "    print(\"tick\", i)\n",
    "\n",
    "await asyncio.gather(*[_ping(i) for i in range(5)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94991394",
   "metadata": {},
   "source": [
    "# ### C4 Parallel Scraping Helper\n",
    "# Uses `gather_scrapers` to fan-out N async tasks with a bounded semaphore.\n",
    "# Total runtime ≈ max(single request latency) instead of sum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.base import gather_scrapers\n",
    "from web_search_sdk.scrapers.duckduckgo_web import _fetch_html as _ddg_fetch, _parse_html as _ddg_parse\n",
    "\n",
    "terms = [\"bitcoin\", \"ethereum\", \"solana\"]\n",
    "async def _parse_wrapper(html: str, term: str, ctx):\n",
    "    return _ddg_parse(html, top_n=5)\n",
    "\n",
    "print(await gather_scrapers(terms, fetch=_ddg_fetch, parse=_parse_wrapper, ctx=ctx_http))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a4452",
   "metadata": {},
   "source": [
    "# ## 12  Closing Notes\n",
    "# • Roadmap → `Progress_Report_v0.2.0.md`  \n",
    "# • Found it useful? **Star** the repo ⭐ & consider contributing – guidelines\n",
    "#   in `CONTRIBUTING.md`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}