{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f234b8",
   "metadata": {},
   "source": [
    "# # Web-Search SDK – End-to-End Demo (V2)\n",
    "# \n",
    "# This notebook shows **how to install, configure and use** the SDK to pull\n",
    "# publicly-available web signals – from simple keyword extraction to\n",
    "# paywall handling and Twitter scraping – in **under 3 minutes**.\n",
    "# \n",
    "# <https://github.com/Gregory-307/web-search-sdk>\n",
    "# \n",
    "# ---\n",
    "# **Tip** Set `OFFLINE_MODE=1` to run everything against fixture HTML – great\n",
    "# for CI or airplane mode!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0d361",
   "metadata": {},
   "source": [
    "# ## 1  Environment Setup  \n",
    "# This cell **bootstraps a completely fresh Colab**:\n",
    "# 1. Installs the Web-Search SDK in *editable* mode (if missing).\n",
    "# 2. Installs the Playwright Python package (if missing).\n",
    "# 3. Downloads headless browser binaries (idempotent).\n",
    "#\n",
    "# Feel free to run it multiple times – each step is safe and will be skipped\n",
    "# when already satisfied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fabbc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, pathlib, importlib.util, importlib\n",
    "\n",
    "ROOT = pathlib.Path(\".\").resolve()\n",
    "\n",
    "# 1️⃣ SDK install ------------------------------------------------------------\n",
    "try:\n",
    "    importlib.util.find_spec(\"web_search_sdk\")  # type: ignore\n",
    "    print(\"web_search_sdk already present – install skipped ✅\")\n",
    "except ImportError:\n",
    "    print(\"Installing Web-Search SDK …\")\n",
    "    subprocess.check_call([\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"pip\",\n",
    "        \"install\",\n",
    "        \"-q\",\n",
    "        \"-e\",\n",
    "        f\"{ROOT}[browser]\",\n",
    "    ])\n",
    "\n",
    "# 2️⃣ Playwright package -----------------------------------------------------\n",
    "try:\n",
    "    import playwright  # type: ignore\n",
    "    print(\"playwright Python package present – install skipped ✅\")\n",
    "except ImportError:\n",
    "    print(\"Installing Playwright Python package …\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"playwright\"])\n",
    "    playwright = importlib.import_module(\"playwright\")  # type: ignore\n",
    "\n",
    "# 3️⃣ Browser binaries -------------------------------------------------------\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"playwright\", \"install\", \"--with-deps\"], stdout=subprocess.DEVNULL)\n",
    "    print(\"Playwright browsers installed ✅\")\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    print(\"Playwright browser install skipped/failed:\", exc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b16784",
   "metadata": {},
   "source": [
    "# ## 2  Quick Smoke Test\n",
    "# Verifies that the freshly-installed package is importable and that the\n",
    "# built-in `smoke_test.py` script runs without network access.  \n",
    "# **Expected output**: version string + a list of top-tokens for the term you\n",
    "# pass on the CLI (defaults inside the script).  This takes <2 s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import runpy, importlib\n",
    "print(\"web_search_sdk version:\", importlib.import_module(\"web_search_sdk\").__version__)\n",
    "runpy.run_path(\"smoke_test.py\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7629d",
   "metadata": {},
   "source": [
    "# ## 3  ScraperContext Cheatsheet\n",
    "# `ScraperContext` is the _single_ configuration object shared by every helper\n",
    "# in the SDK.  It controls:\n",
    "# • HTTP headers, timeouts & retries  \n",
    "# • Proxy / custom User-Agent pools  \n",
    "# • Whether to launch a headless browser fallback (Selenium / Playwright)  \n",
    "# • Verbose logging for debugging\n",
    "#\n",
    "# The cell below instantiates three ready-made contexts to reuse in later\n",
    "# examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.base import ScraperContext\n",
    "ctx_http  = ScraperContext()\n",
    "ctx_selen = ScraperContext(use_browser=True, browser_type=\"selenium\", debug=False)\n",
    "ctx_play  = ScraperContext(use_browser=True, browser_type=\"playwright_stealth\")\n",
    "ctx_http, ctx_selen, ctx_play\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127b75a",
   "metadata": {},
   "source": [
    "# ### 4.1 DuckDuckGo – Top Words\n",
    "# Primary engine: zero CAPTCHA risk, lightweight HTML.  Returns top-N tokens\n",
    "# from the SERP snippets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f035f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.duckduckgo_web import duckduckgo_top_words\n",
    "print(await duckduckgo_top_words(\"bitcoin swing\", ctx_http, top_n=15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8fdac4",
   "metadata": {},
   "source": [
    "# ### 4.2 Wikipedia – Top Words\n",
    "# Low-latency and highly reliable.  Good sanity-check source for any term.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94788f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.wikipedia import wikipedia_top_words\n",
    "print(await wikipedia_top_words(\"bitcoin\", ctx_http, top_n=15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a60a2f",
   "metadata": {},
   "source": [
    "# ### 4.3 RelatedWords – Synonyms\n",
    "# Expands the seed term via semantic similarity API; useful for idea\n",
    "# generation or keyword expansion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f45be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.related import related_words\n",
    "_syn = await related_words(\"bitcoin\", ctx_http)\n",
    "print(_syn[:15])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f4a4e",
   "metadata": {},
   "source": [
    "# ### 4.4 Google News RSS – Keywords\n",
    "# Headlines surface fresh jargon earlier than static pages – this parser\n",
    "# extracts frequent tokens from the Google News RSS feed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a0d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.news import google_news_top_words\n",
    "print(await google_news_top_words(\"bitcoin\", ctx_http, top_n=15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052371d",
   "metadata": {},
   "source": [
    "# ## 5  Google SERP Fallback *(optional)*\n",
    "# Heavy and may hit CAPTCHA – **runs by default**. Set `DISABLE_GOOGLE=1` to skip in CI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f82399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Skip only when explicitly disabled\n",
    "if os.getenv(\"DISABLE_GOOGLE\") == \"1\":\n",
    "    print(\"[skipped] DISABLE_GOOGLE env var set\")\n",
    "else:\n",
    "    from web_search_sdk.scrapers.google_web import google_web_top_words\n",
    "    print(await google_web_top_words(\"bitcoin swing\", ctx_play, top_n=15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f5368",
   "metadata": {},
   "source": [
    "# ## 6  Paywall Article Retrieval\n",
    "# Shows automatic switch to Playwright when a JS-heavy paywall blocks simple\n",
    "# HTTP.  Trimmed article body is printed to keep output concise.  Skips when\n",
    "# `OFFLINE_MODE=1`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870229af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.paywall import fetch_bloomberg\n",
    "if os.getenv(\"OFFLINE_MODE\"):\n",
    "    print(\"[skipped] Offline mode – using fixtures\")\n",
    "else:\n",
    "    art = await fetch_bloomberg(\"https://www.bloomberg.com/news/articles/2023-12-01/bitcoin-price-breaks-40k\", ctx_play)\n",
    "    print(art[:400], \"…\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df706c",
   "metadata": {},
   "source": [
    "# ## 7  Twitter Login & Sample Scrape *(experimental)*\n",
    "# **Requires** env vars `TW_EMAIL`, `TW_PASS`.  Runs automatically when creds are present; otherwise skipped.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run when credentials are provided (no extra flag needed)\n",
    "tw_user = os.getenv(\"TW_EMAIL\")\n",
    "tw_pass = os.getenv(\"TW_PASS\")\n",
    "\n",
    "if tw_user and tw_pass:\n",
    "    # Minimal inline Playwright demo (pseudo-code for brevity)\n",
    "    from playwright.async_api import async_playwright  # type: ignore\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(\"https://twitter.com/login\")\n",
    "        await page.fill(\"input[name='text']\", tw_user)\n",
    "        await page.press(\"input[name='text']\", \"Enter\")\n",
    "        await page.fill(\"input[name='password']\", tw_pass)\n",
    "        await page.press(\"input[name='password']\", \"Enter\")\n",
    "        await page.wait_for_selector(\"article\")\n",
    "        html = await page.content()\n",
    "        print(html[:500], \"…\")\n",
    "        await browser.close()\n",
    "else:\n",
    "    print(\"[skipped] Provide TW_EMAIL and TW_PASS env vars to enable Twitter demo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a49925",
   "metadata": {},
   "source": [
    "# ## 8  Output Utilities\n",
    "# Lightweight helpers that write structured results to JSON/CSV.  Both create\n",
    "# parent folders automatically and support **append** mode for easy logging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adfdff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.utils.output import to_json, to_csv\n",
    "import json, pathlib\n",
    "pathlib.Path(\"out\").mkdir(exist_ok=True)\n",
    "\n",
    "json_path = \"out/demo_tokens.json\"\n",
    "to_json([\"btc\", \"eth\", \"doge\"], json_path, append=False)\n",
    "print(\"Wrote\", json_path, \"bytes:\", pathlib.Path(json_path).stat().st_size)\n",
    "\n",
    "csv_path = \"out/demo_tokens.csv\"\n",
    "to_csv([{\"term\": \"btc\", \"hits\": 120}], csv_path, append=False)\n",
    "print(\"Wrote\", csv_path, \"bytes:\", pathlib.Path(csv_path).stat().st_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03c344",
   "metadata": {},
   "source": [
    "# ## 9  Text Helpers\n",
    "# Tokenisation + stop-word removal + frequency counter in one line each.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.utils.text import tokenise, remove_stopwords, most_common\n",
    "raw = \"Bitcoin's all-time high price sparks FOMO!\"\n",
    "print(\"tokens:\", tokenise(raw))\n",
    "print(\"no stopwords:\", remove_stopwords(tokenise(raw)))\n",
    "print(\"top:\", most_common(tokenise(raw), 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0750cd",
   "metadata": {},
   "source": [
    "# ## 10  Rate-Limit Decorator\n",
    "# Async token-bucket decorator – guarantees you never exceed X calls / period.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from web_search_sdk.utils.rate_limit import rate_limited\n",
    "\n",
    "@rate_limited(calls=2, period=1.0)\n",
    "async def _ping(i: int):\n",
    "    print(\"tick\", i)\n",
    "\n",
    "await asyncio.gather(*[_ping(i) for i in range(5)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a1655",
   "metadata": {},
   "source": [
    "# ## 11  Parallel Scraping Example\n",
    "# Uses `gather_scrapers` to fan-out N async tasks with a bounded semaphore.\n",
    "# Total runtime ≈ max(single request latency) instead of sum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67066c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_sdk.scrapers.base import gather_scrapers\n",
    "from web_search_sdk.scrapers.duckduckgo_web import _fetch_html as _ddg_fetch, _parse_html as _ddg_parse\n",
    "\n",
    "terms = [\"bitcoin\", \"ethereum\", \"solana\"]\n",
    "async def _parse_wrapper(html: str, term: str, ctx):\n",
    "    return _ddg_parse(html, top_n=5)\n",
    "\n",
    "print(await gather_scrapers(terms, fetch=_ddg_fetch, parse=_parse_wrapper, ctx=ctx_http))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76bd50b",
   "metadata": {},
   "source": [
    "# ## 12  Closing Notes\n",
    "# • Roadmap → `Progress_Report_v0.2.0.md`  \n",
    "# • Found it useful? **Star** the repo ⭐ & consider contributing – guidelines\n",
    "#   in `CONTRIBUTING.md`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}